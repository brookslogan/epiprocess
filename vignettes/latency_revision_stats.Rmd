---
title: Calculating latency and revision statistics
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Calculating latency and revision statistics}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

# Load data

```{r}
library(epidatr)
library(epiprocess)
library(dplyr)
library(tidyr)
library(ggplot2)
library(plotly)

# For reproducibility, we'll work with data reporting through some fixed issue
# we've already observed. For better reproducibility, that issue should be at
# least two days ago (from today), to incorporate any "hotfixes" to that data
# and allow up to a day for database replication delays:
hhs_flu_hosp_analysis_issue = as.Date("2023-06-13") - 2L

# We'll query version data for this data set from the Delphi Epidata API using
# epidatr, caching the result so that we can re-run this chunk repeatedly a bit
# faster, while offline, and without placing undue load on the API servers.
hhs_flu_hosp_issue_data_path = "hhs_flu_hosp_issue_data.rds"
if (!file.exists(hhs_flu_hosp_issue_data_path)) {
  cat("Fetching.\n")
  hhs_flu_hosp_issue_data =
    covidcast(
      data_source = "hhs", signals = "confirmed_admissions_influenza_1d",
      geo_type = "state", time_type = "day",
      geo_values = "*",
      # all `time_value`s:
      time_values = "*",
      # all issues through the analysis issue
      issues = epirange(12340101, hhs_flu_hosp_analysis_issue)
    ) %>%
    fetch()
  saveRDS(hhs_flu_hosp_issue_data, hhs_flu_hosp_issue_data_path)
  writeLines(c("License: Public Domain US Government",
               "Accessed via COVIDcast Epidata API"),
             "LICENSE_hhs_flu_hosp_issue_data.txt")
} else {
  cat("Using cache.\n")
  hhs_flu_hosp_issue_data = readRDS(hhs_flu_hosp_issue_data_path)
}

# Remove some data from a mostly-NA/0/low regime, and put version data into
# the `epi_archive` format:
hhs_flu_hosp_start_time_value = as.Date("2020-12-01")
hhs_flu_hosp_archive = hhs_flu_hosp_issue_data %>%
  filter(time_value >= hhs_flu_hosp_start_time_value) %>%
  select(geo_value, time_value, version = issue, admissions = value) %>%
  as_epi_archive(compactify = TRUE)
```

# Latency statistics

There are several notions of latency, some of which include:
- "Nominal latency": the difference between the max `time_value` available for
  some location, as of some version
- "Nonmissing latency": also considers explicitly-recorded `NA`s and any skipped
  `time_value`s as latent
- "Nonmissing nonzero latency": considers explicitly-recorded `NA`s, skipped
  `time_value`s, and zeros as latent
- "Nonmissing nonduplicate latency": considers `NA`s, skipped `time_value`s, and
  duplications of the most recent non-`NA` value to be latent
- "Nonmissing nonzero nonduplicate latency": considers `NA`s, skipped
  `time_value`s, zeros, and duplications of the most recent non-`NA`, nonzero
  value to be latent

We'll calculate all the above definitions of latency using `epix_slide()`:
```{r latency-stats, cache=TRUE}
n_bools_true_at_tail = function(x) {
  match(FALSE, rev(x), nomatch = length(x) + 1L) - 1L
}

last_or_na = function(x) {
  if (length(x) != 0L) {
    x[[length(x)]]
  } else {
    vctrs::vec_cast(NA, x)
  }
}

latency_stats = function(window_data, group_key, ref_time_value) {
  window_data %>%
    group_by(geo_value) %>%
    complete(time_value = full_seq(time_value, period = 1L)) %>%
    ungroup() %>%
    mutate(
      is_na = is.na(admissions),
      is_na0 = is_na | admissions == 0L
    ) %>%
    group_by(geo_value) %>%
    mutate(
      is_na_same = is_na |
        admissions == last_or_na(admissions[!is_na]),
      is_na0same = is_na0 |
        admissions == last_or_na(admissions[!is_na0])
    ) %>%
    summarize(
      nominal_latency = as.integer(ref_time_value - max(time_value)),
      na_latency = n_bools_true_at_tail(is_na) + nominal_latency,
      na0_latency = n_bools_true_at_tail(is_na0) + nominal_latency,
      na_dup_latency =
        # (don't count NAs before the first copy of the last non-na)
        sum(cumsum(is_na_same[rev(cumsum(rev(!is_na_same))) == 0L]) != 0L) - 1L,
      na0dup_latency =
        # (don't count NAs and 0s before the first copy of the last non-na0)
        sum(cumsum(is_na0same[rev(cumsum(rev(!is_na0same))) == 0L]) != 0L) - 1L
    )
}

hhs_flu_hosp_latency_by_version_geo =
  hhs_flu_hosp_archive %>%
  epix_slide(
    latency_stats,
    before = 27L, # don't calculate latency more than this far back
    names_sep = NULL # don't add "slide_value_" colname prefixes
  ) %>%
  rename(version = time_value)
```

## Finer-grained visualizations of different notions of latency

Nominal latency, where it is over 2 days:
```{r, out.width = "60em", out.height="60em"}
plt =
  hhs_flu_hosp_latency_by_version_geo %>%
  mutate(nominal_latency = case_when(
    nominal_latency <= 2 ~ NA,
    TRUE ~ nominal_latency
  )) %>%
  ggplot(aes(version, geo_value, fill = nominal_latency)) +
  geom_raster() +
  scale_fill_viridis_c(na.value = "transparent") +
  theme_bw() +
  scale_x_date(date_labels = "%b %Y") +
  xlab("Version") +
  ylab("HHS Flu Hospitalization Nominal Latency")
ggplotly(plt)
```

Nonmissing latency, where it is over 2 days:
```{r, out.width = "60em", out.height="60em"}
plt =
  hhs_flu_hosp_latency_by_version_geo %>%
  mutate(na_latency = case_when(
    na_latency <= 2 ~ NA,
    TRUE ~ na_latency
  )) %>%
  ggplot(aes(version, geo_value, fill = na_latency)) +
  geom_raster() +
  scale_fill_viridis_c(na.value = "transparent") +
  theme_bw() +
  scale_x_date(date_labels = "%b %Y") +
  xlab("Version") +
  ylab("HHS Flu Hospitalization Nonmissing Latency")
ggplotly(plt)
```

Nonmissing nonzero latency, where it is over 2 days:
```{r, out.width = "60em", out.height="60em"}
plt =
  hhs_flu_hosp_latency_by_version_geo %>%
  mutate(na0_latency = case_when(
    na0_latency <= 2 ~ NA,
    TRUE ~ na0_latency
  )) %>%
  ggplot(aes(version, geo_value, fill = na0_latency)) +
  geom_raster() +
  scale_fill_viridis_c(na.value = "transparent") +
  theme_bw() +
  scale_x_date(date_labels = "%b %Y") +
  xlab("Version") +
  ylab("HHS Flu Hospitalization Nonmissing Nonzero Latency")
ggplotly(plt)
```


Nonmissing nonduplicate latency, where it is over **3** days:
```{r, out.width = "60em", out.height="60em"}
plt =
  hhs_flu_hosp_latency_by_version_geo %>%
  mutate(na_dup_latency = case_when(
    na_dup_latency <= 3 ~ NA,
    TRUE ~ na_dup_latency
  )) %>%
  ggplot(aes(version, geo_value, fill = na_dup_latency)) +
  geom_raster() +
  scale_fill_viridis_c(na.value = "transparent") +
  theme_bw() +
  scale_x_date(date_labels = "%b %Y") +
  xlab("Version") +
  ylab("HHS Flu Hospitalization Nonmissing Nonduplicate Latency")
ggplotly(plt)
```

Nonmissing nonzero nonduplicate latency, where it is over **3** days:
```{r, out.width = "60em", out.height="60em"}
plt =
  hhs_flu_hosp_latency_by_version_geo %>%
  mutate(na0dup_latency = case_when(
    na0dup_latency <= 3 ~ NA,
    TRUE ~ na0dup_latency
  )) %>%
  ggplot(aes(version, geo_value, fill = na0dup_latency)) +
  geom_raster() +
  scale_fill_viridis_c(na.value = "transparent") +
  theme_bw() +
  scale_x_date(date_labels = "%b %Y") +
  xlab("Version") +
  ylab("HHS Flu Hospitalization Nonmissing Nonzero Nonduplicate Latency")
ggplotly(plt)
```

## Plotting individual time series snapshots of interest

There's a time span of high latency according to some definitions, e.g., for DE as of 2023-05-11; let's see what the time series looked like as of that version:
```{r}
plt =
  hhs_flu_hosp_archive %>%
  epix_as_of(as.Date("2023-05-11"),
             # plot just a year of data:
             min_time_value = as.Date("2022-05-12")) %>%
  filter(geo_value == "de") %>%
  complete(time_value = full_seq(time_value, period = 1L)) %>%
  ggplot(aes(time_value, admissions)) +
  geom_point()
ggplotly(plt)
```
We can see at later time values that the number of admissions alternates between 0 most days and 1 other days.  In some definitions of latency, we consider all but the first 1

## Some summary statistics related to the nonmissing latency:

```{r, fig.width = 7, fig.height = 5}
hhs_flu_hosp_latency_by_version_geo %>%
  ggplot(aes(na_latency)) +
  geom_histogram(binwidth = 1L)
```

Over time:
```{r, fig.width = 7, fig.height = 5}
hhs_flu_hosp_latency_by_version_geo %>%
  group_by(version) %>%
  summarize(mean_na_latency = mean(na_latency)) %>%
  ggplot(aes(version, mean_na_latency)) +
  geom_line() +
  expand_limits(y=0) +
  xlab("Version") +
  ylab("Mean Nonmissing Latency Across Locations")
```

# Revision statistics

How accurate is `k`-day-old data on average?
```{r, fig.width = 7, fig.height = 5}
max_analysis_lag = 30L

hhs_flu_hosp_version_lag_data =
  hhs_flu_hosp_archive %>%
  epix_slide(
    ~ .x,
    before = max_analysis_lag,
    as_list_col = TRUE
  ) %>%
  rename(version = time_value) %>%
  unnest(slide_value) %>%
  mutate(version_lag = as.integer(version - time_value))

hhs_flu_hosp_latest_snapshot =
  hhs_flu_hosp_archive %>%
  epix_as_of(.$versions_end)

left_join(hhs_flu_hosp_version_lag_data,
          hhs_flu_hosp_latest_snapshot %>%
            filter(time_value <= hhs_flu_hosp_analysis_issue - max_analysis_lag * 2L),
          by = c("geo_value", "time_value"),
          suffix = c("_lagged", "_latest")) %>%
  tidyr::drop_na(c(admissions_lagged, admissions_latest)) %>%
  group_by(version_lag) %>%
  summarize(MAPE1 = 100*mean(abs((admissions_latest - admissions_lagged)/(admissions_latest + 1)))) %>%
  ggplot(aes(version_lag, MAPE1)) +
  geom_line()

left_join(hhs_flu_hosp_version_lag_data,
          hhs_flu_hosp_latest_snapshot %>%
            filter(time_value <= hhs_flu_hosp_analysis_issue - max_analysis_lag * 2L),
          by = c("geo_value", "time_value"),
          suffix = c("_lagged", "_latest")) %>%
  tidyr::drop_na(c(admissions_lagged, admissions_latest)) %>%
  group_by(version_lag) %>%
  summarize(MdAPE1 = 100*median(abs((admissions_latest - admissions_lagged)/(admissions_latest + 1)))) %>%
  ggplot(aes(version_lag, MdAPE1)) +
  geom_line()

left_join(hhs_flu_hosp_version_lag_data,
          hhs_flu_hosp_latest_snapshot %>%
            filter(time_value <= hhs_flu_hosp_analysis_issue - max_analysis_lag * 2L),
          by = c("geo_value", "time_value"),
          suffix = c("_lagged", "_latest")) %>%
  tidyr::drop_na(c(admissions_lagged, admissions_latest)) %>%
  group_by(version_lag) %>%
  summarize(Q3APE1 = 100*quantile(abs((admissions_latest - admissions_lagged)/(admissions_latest + 1)), 0.75)) %>%
  ggplot(aes(version_lag, Q3APE1)) +
  geom_line()

left_join(hhs_flu_hosp_version_lag_data,
          hhs_flu_hosp_latest_snapshot %>%
            filter(time_value <= hhs_flu_hosp_analysis_issue - max_analysis_lag * 2L),
          by = c("geo_value", "time_value"),
          suffix = c("_lagged", "_latest")) %>%
  tidyr::drop_na(c(admissions_lagged, admissions_latest)) %>%
  group_by(version_lag) %>%
  reframe(
    tau = (1:9)/10,
    QtauAPE1 = 100*quantile(abs((admissions_latest - admissions_lagged)/(admissions_latest + 1)), tau)
  ) %>%
  mutate(tau = as.factor(format(tau, digits = 3))) %>%
  ggplot(aes(version_lag, QtauAPE1, colour = tau, group = tau, label = tau)) +
  geom_line() +
  scale_colour_discrete(
    guide = guide_legend(reverse = TRUE,
                         override.aes = list(label = ""))
  )
```
