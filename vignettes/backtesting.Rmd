---
title: Backtesting forecasters and other models
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Backtesting forecasters and other models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

### Overview, and why to use `epix_slide()`

The `epiprocess` package provides two functions that are useful for backtesting
forecasts and other models: `epi_slide()` and `epix_slide()`; however, we
recommend always using the latter, because:

* `epix_slide()` faithfully reproduces latency in data reporting, while
  `epi_slide()` does not; and
* `epix_slide()` faithfully reproduces the version of each available observation
  that would have been available in real time, while `epi_slide()` does not.

Using `epi_slide()` or a similar approach instead would typically produce overly
optimistic retrospective forecast evaluations, and might miss some run-time
errors that would have occurred in real time due to irregular latency in data
reporting. Thus, it is essential to use a version-aware approach like
`epix_slide()` for backtesting evaluations.

Additionally, `epix_slide()` can be used to build and evaluate version-aware
models that use data on historical data revisions to try to improve their
accuracy.

### Smaller reproducibility issues that may remain

Even using version-aware approaches like `epix_slide()`, there may still be some
smaller reproducibility issues based on the data set/provider:

* If the `version` tags are not datetime resolution, it is unclear what the
  input to the forecaster would have looked like at a particular time. For
  example, if `version`s are `Date`s, it can be unclear which (if any) data from
  version `2023-01-01` were available at 8am Eastern Time on `2023-01-01`.
* Depending on the data source, the `version` tags may not reflect the exact
  time that the data actually was available. For example, an upstream data
  provider might have some different meaning for its `version` column, or might
  use database replicas which may need time to process updates and make them
  available to all data consumers.
* Even though we generally think of available version data as fixed in stone,
  this isn't necessarily the case. For example, an upstream version data
  provider may publish some initial form of version `2023-01-01` data early in
  the morning of `2023-01-01`, but discover that it contains an error, and issue
  a hotfix later in the day that overwrites the `2023-01-01` version data with a
  correction (but still using the exact same version tag); in this situation,
  the early-morning version wouldn't be reproducible if performing an analysis
  at a later date.

# Latency issues: most recent time available vs. `ref_time_value`

Some forecasters select test-time predictors using the maximum time value
available (either overall, or separately for each individual time series for
different geographical/demographical units). E.g., using default settings,
`epipredict::arx_forecaster` will:
- Select test-time predictors for each time series corresponding to
  `max_time_value - lags` for that time series.
  
 FIXME probably wrong/imprecise.  Need to consider shifts involved.
  
- Guess that the `forecast_date` is the maximum of these `max_time_value`s over
  all the time series. Guess that the `target_date` is `forecast_date + ahead`.

We'll demonstrate some issues that occur using these default settings, first
when dealing with a single time series, and then when dealing with multiple time
series with potentially different reporting latencies.

### Single location, fixed latency

First, we'll construct a synthetic data set with a single location where one
particular lag of a predictor is extremely useful, while all the others are not,
and show how ignoring latency can hurt forecast accuracy.

```{r}
library(dplyr)
library(tidyr)
library(purrr)
library(epiprocess)
library(epipredict)

# Make RNG reproducible:
invisible(withr::local_rng_version("3.0.0"))
invisible(withr::local_seed(42L))

# Settings for our synthetic data set and forecasting task:
great_lag_setting = 8L
target_ahead = 14L
fixed_latency = 3L
nrow_simulated = 100L + great_lag_setting + target_ahead

edf = tibble(
  geo_value = rep("nc", nrow_simulated),
  time_value = as.Date("2021-01-01") + seq(0L, nrow_simulated - 1L),
  predictor = rnorm(nrow_simulated),
  outcome =
    dplyr::lag(predictor, great_lag_setting + target_ahead) +
    0.01 * rnorm(nrow_simulated)
) %>%
  drop_na(outcome) %>%
  as_epi_df()

ea = edf %>%
  mutate(version = time_value + fixed_latency) %>%
  as_epi_archive()

latency_unaware_predictions = function(group_data, group_key) {
  group_data %>%
    arx_forecaster(
      "outcome", "predictor",
      args_list = arx_args_list(
        lags = 0:9,
        ahead = target_ahead
      )
    ) %>%
    `[[`("predictions") %>%
    rename(max_time_value = time_value)
}

actual_forecast_date = ea$versions_end

ea %>%
  epix_slide(
    ref_time_values = actual_forecast_date,
    before = 36500L, # use an extremely large time window = no time windowing
    f = latency_unaware_predictions,
    names_sep = NULL # don't prefix output column names
  )

cat("Actual forecast date:", toString(actual_forecast_date), "\n")
```

TODO finish

TODO what effect this has in reality

TODO also give example where this reproduces real-time errors for systems that
are more rigid and calculate from real `forecast_date` but encounter something not there

# latency issue: mixed latency

# pseudoprospective forecasting

# revision MAPE curve

# version-aware forecasting

Version-aware forecasting:
```{r}
library(epidatr)

# For reproducibility, we'll work with data reporting through some fixed issue
# we've already observed. For better reproducibility, that issue should be at
# least two days ago (from today), to incorporate any "hotfixes" to that data
# and allow up to a day for database replication delays:
ga_hhs_analysis_issue = as.Date("2023-04-05") - 2L
# Fetch issue data using `epidatr`; cache the response for politeness, and also
# note the data license for the requested signal.
# TODO try cachem
ga_hhs_issue_data_path = "ga_hhs_issue_data.rds"
if (!file.exists(ga_hhs_issue_data_path)) {
  ga_hhs_issue_data =
    covidcast(
      data_source = "hhs", signal = "confirmed_admissions_influenza_1d",
      geo_type = "state", time_type = "day",
      geo_values = "ga",
      # all `time_value`s:
      time_values = "*",
      # all issues through the analysis issue
      issues = epirange(12340101, ga_hhs_analysis_issue)
    ) %>%
    fetch_tbl()
  saveRDS(ga_hhs_issue_data, ga_hhs_issue_data_path)
  writeLines(c("License: Public Domain US Government",
               "Accessed via COVIDcast Epidata API"),
             "LICENSE_ga_hhs_issue_data.txt")
} else {
  ga_hhs_issue_data = readRDS(ga_hhs_issue_data_path)
}

# Remove some data from a mostly-NA/0/low regime, and put version data into
# the `epi_archive` format:
hhs_start_time_value = as.Date("2020-12-01")
ga_hhs_archive = ga_hhs_issue_data %>%
  filter(time_value >= hhs_start_time_value) %>%
  select(geo_value, time_value, version = issue, admissions = value) %>%
  as_epi_archive(compactify = TRUE)

# We'll experiment with a pseudoprospective forecast as of a single past
# `forecast_date`. Later, we can apply this to many `forecast_date`s using an
# `epix_slide()`.
forecast_date = ga_hhs_analysis_issue - 7L

# Get what the archive would have looked like as of the `forecast_date`:
forecast_date_archive = ga_hhs_archive %>%
  epix_as_of(forecast_date, all_versions = TRUE)
# Get what the snapshot as of the `forecast_date` would have looked like:
forecast_date_edf = forecast_date_archive %>%
  epix_as_of(.$versions_end)
# Reporting latency for this signal in GA on this `forecast_date`:
forecast_date_latency = as.integer(forecast_date - max(forecast_date_edf$time_value))

# Our test-time predictors will correspond to `time_value`s in `forecast_date -
# forecast_date_latency - c(0L, 7L, 14L)`. We'll match them to analogous
# training data based on subbing in diffent `training_forecast_date`s. Calculate
# the desired lags relative to the `forecast_date`, and assign them names to be
# used later:
test_predictor_lags_rel_max_t = c(x1 = 0L, x2 = 7L, x3 = 14L)
predictor_lags_rel_fcst_date = forecast_date_latency + test_predictor_lags_rel_max_t
# (Edge case: while we probably want to include `forecast_date -
# forecast_date_latency - 0L` to take advantage of the most recent `time_value`
# available at test time, this could potentially lead to errors if a data source
# starts reporting with lower latency, because we will have trouble matching
# with analogous training data.)

# We also want to limit our training instances to `training_forecast_date`s with
# the same weekday as our `forecast_date`:
training_and_testing_issues_start = max(
  # try to go all the way back to the first version in the archive,
  min(forecast_date_archive$DT$version),
  # that we expect to possibly contain observations for the desired lags
  hhs_start_time_value + max(predictor_lags_rel_fcst_date)
)
# Prepare our training and testing `forecast_date`s, making sure to match the
# wday of the desired `forecast_date`:
training_and_testing_issues = rev(seq(forecast_date, training_and_testing_issues_start, -7))

# Find analogous predictor values using `epix_slide()`; get both training and
# testing data simultaneously here as it ends up more convenient than trying to
# separately fetch test data from `forecast_date_edf`:
version_aware_predictors =
  ga_hhs_archive %>%
  epix_slide(
    # Our `training_forecast_date`s + testing `forecast_date`
    ref_time_values = training_and_testing_issues,
    # For performance purposes, apply a time window that includes just enough
    # `time_value`s that we can extract our predictors:
    before = max(predictor_lags_rel_fcst_date),
    function(edf, gk) {
      # * `edf` is a time-windowed snapshot.
      # * `gk` is the "group key". If we were performing a grouped slide, this
      #   could potentially be helpful.  We won't need it here.
      ref_time_value = attr(edf, "metadata")$as_of
      # `ref_time_value` is our training/actual `forecast_date`.
      edf %>%
        # We must make sure that there are no gaps in the `time_value`s to make
        # `dplyr::lag` work properly. Additionally, we will extend the
        # `time_value`s to the `ref_time_value`, so we can just use `dplyr::lag`
        # plus filter to `time_value == ref_time_value` to get our desired data.
        complete(
          geo_value,
          time_value = full_seq(c(time_value, ref_time_value), period=1L)
        ) %>%
        # We'd need this `group_by` if we actually had multiple `geo_value`s:
        group_by(geo_value) %>%
        # Prepare columns x1, x2, x3 by `dplyr::lag`ging `admissions`:
        mutate(map_dfc(
          predictor_lags_rel_fcst_date,
          # test_predictor_lags_rel_max_t,
          ~ dplyr::lag(admissions, .x)
        )) %>%
        # We're done with the extra rows used to calculate the lags:
        filter(time_value == ref_time_value) %>%
        # Make sure we actually output a row so joins are easier:
        complete(time_value = ref_time_value) %>%
        ungroup() %>%
        select(geo_value, x1, x2, x3)
    },
    names_sep = NULL # don't prefix the output column names
  )

# Combine the version-aware predictors with the latest available version, from
# which we'll calculate targets as of the `forecast_date`. By predicting
# fully/mostly revised targets from real-time predictors, we are making a
# revision-aware forecast. For some models or evaluation schemes, we might want
# to downweight or discard the most recent data from our training sets, or take
# other more complex approaches. But for this model, since we don't use a
# training window, the current approach seems reasonable.
training_and_testing_edf =
  left_join(version_aware_predictors,
            # shift outcome data so `epipredict` indexes relative to forecast date:
            forecast_date_edf %>%
              mutate(time_value = time_value + forecast_date_latency),
            by = c("geo_value", "time_value")) %>%
  as_epi_df(as_of = forecast_date)
# (Caution: since we matched wdays, this is essentially a weekly data set, and
# certain forecasters that use `dplyr::lag` or `dplyr::ahead` without
# considering `time_value` will work differently than they would on daily data.
# This isn't the case with `epipredict` below.)

# Use `epipredict` canned forecaster `arx_forecaster`.
forecaster_output = training_and_testing_edf %>%
  arx_forecaster(
    outcome = "admissions",
    predictors = c("x1", "x2", "x3"),
    args_list = arx_args_list(
      lags = 0L, # the lags are already baked into x1, x2, x3
      # ahead = 14L,
      # ahead = 7L,
      ahead = 0L,
      # Use defaults for everything else.
      )
  )


library(ggplot2)
training_and_testing_edf %>%
  ggplot(aes(x1, admissions)) %>%
  `+`(geom_point()) %>%
  `+`(geom_jitter()) %>%
  `+`(geom_abline(slope = 1, intercept = 0, linetype="dotted"))

training_and_testing_edf %>%
  filter(!is.na(x1) & !is.na(admissions)) %>%
  with(sum(admissions) / sum(x1))
# Generally initial observations are revised downward.

# Our fit coefficients:
forecaster_output$epi_workflow %>%
  parsnip::extract_fit_engine()

# Actual backcasts are in `forecaster_output$predictions`
```

# pseudoprospective version-aware forecasting
